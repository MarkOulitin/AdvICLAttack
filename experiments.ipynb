{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "text_attack_cache_dir = os.path.join(os.getcwd(), 'text-attack')\n",
    "os.environ['TA_CACHE_DIR'] = text_attack_cache_dir\n",
    "if not os.path.isdir(text_attack_cache_dir):\n",
    "    os.mkdir(text_attack_cache_dir)\n",
    "from datasets import load_dataset\n",
    "from textattack.attack_recipes.textbugger_li_2018 import TextBuggerLi2018\n",
    "from textattack.models.wrappers.huggingface_model_wrapper import HuggingFaceModelWrapper\n",
    "from textattack.models.wrappers.model_wrapper import ModelWrapper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    LlamaConfig\n",
    "    ) \n",
    "from typing import List, Tuple\n",
    "from textattack import Attack\n",
    "from textattack.constraints.pre_transformation import (\n",
    "    RepeatModification,\n",
    "    StopwordModification,\n",
    ")\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "from textattack.transformations import (\n",
    "    Transformation,\n",
    "    CompositeTransformation,\n",
    "    WordSwapEmbedding,\n",
    "    WordSwapHomoglyphSwap,\n",
    "    WordSwapNeighboringCharacterSwap,\n",
    "    WordSwapRandomCharacterDeletion,\n",
    "    WordSwapRandomCharacterInsertion,\n",
    ")\n",
    "from textattack.attack_recipes import AttackRecipe\n",
    "from textattack.constraints import Constraint\n",
    "import utils\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.11s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\", cache_dir=\"./models/casual-llama\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"huggyllama/llama-7b\", cache_dir=\"./models/casual-llama\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLConstraint(Constraint):\n",
    "\n",
    "    def __init__(self, pattern):\n",
    "        super().__init__(compare_against_original=True)\n",
    "        self._pattern = pattern\n",
    "\n",
    "    def _check_constraint(self, transformed_text, reference_text) -> bool:\n",
    "        reference_matches = re.findall(self._pattern, reference_text)\n",
    "        if reference_matches:\n",
    "            last_match_reference = reference_matches[-1]\n",
    "            start_index_reference = reference_text.rindex(last_match_reference)\n",
    "            \n",
    "            transformed_matches = re.findall(self._pattern, transformed_text)\n",
    "            if transformed_matches:\n",
    "                last_match_transformed = transformed_matches[-1]\n",
    "                start_index_transformed = transformed_text.rindex(last_match_transformed)\n",
    "                \n",
    "                # return true if the suffix is the same\n",
    "                return transformed_text[start_index_transformed:] == reference_text[start_index_reference:]\n",
    "            else:\n",
    "                # no match in transformed text\n",
    "                return False\n",
    "        else:\n",
    "            # no match in reference text\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLTextBugger(AttackRecipe):\n",
    "    @staticmethod\n",
    "    def build(model_wrapper, pattern):\n",
    "        transformation = CompositeTransformation([\n",
    "            WordSwapRandomCharacterInsertion(\n",
    "                random_one=True,\n",
    "                letters_to_insert=\" \",\n",
    "                skip_first_char=True,\n",
    "                skip_last_char=True,\n",
    "            ),\n",
    "            WordSwapRandomCharacterDeletion(\n",
    "                random_one=True, skip_first_char=True, skip_last_char=True\n",
    "            ),\n",
    "            WordSwapNeighboringCharacterSwap(\n",
    "                random_one=True, skip_first_char=True, skip_last_char=True\n",
    "            ),\n",
    "            WordSwapHomoglyphSwap(),\n",
    "            WordSwapEmbedding(max_candidates=5),\n",
    "        ])\n",
    "        constraints = [\n",
    "            RepeatModification(),\n",
    "            StopwordModification(),\n",
    "            UniversalSentenceEncoder(threshold=0.8),\n",
    "            ICLConstraint(pattern),\n",
    "        ]\n",
    "        goal_function = UntargetedClassification(model_wrapper)\n",
    "        search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
    "        \n",
    "        return Attack(goal_function, constraints, transformation, search_method)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (d:/Cyber-final-project/data/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 746.98it/s]\n"
     ]
    }
   ],
   "source": [
    "sst2_dataset = load_dataset(\"sst2\", cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Template:\n",
    "    def __init__(self, template: str, instruction: str) -> None:\n",
    "        if '{example}' not in template:\n",
    "            raise Exception('example placeholder not in template')\n",
    "        if '{label}' not in template:\n",
    "            raise Exception('label placeholder not in template')\n",
    "        self.template: str = template\n",
    "        self.instruction: str = instruction\n",
    "    \n",
    "    def apply(self, examples: List[str], labels: List[str]) -> str:\n",
    "        accumulator = []\n",
    "        for example, label in zip(examples, labels): \n",
    "            accumulator.append(\n",
    "                self.template.format(\n",
    "                    example=example,\n",
    "                    label=label\n",
    "                )\n",
    "            )\n",
    "        return f'{self.instruction}\\n\\n' + '\\n'.join(accumulator)\n",
    "\n",
    "class ICLSample:\n",
    "\n",
    "    def __init__(self, examples: List[str], labels: List[str], template: Template, test_sample: str) -> None:\n",
    "        if len(examples) != len(labels):\n",
    "            raise Exception('examples and labels length are not the same')\n",
    "        self.examples: List[str] = examples\n",
    "        self.labels: List[str] = labels\n",
    "        self.test_sample: str = test_sample\n",
    "        self.template: Template = template\n",
    "\n",
    "    def to_text(self) -> str:\n",
    "        examples = self.examples + [self.test_sample]\n",
    "        labels = self.labels + ['_']\n",
    "        return self.template.apply(examples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_into_chunks(array: list, chunk_size: int):\n",
    "    chunks = []\n",
    "    for i in range(0, len(array), chunk_size):\n",
    "        chunk = array[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_config = {\n",
    "    'pattern': r\"Review: .+?\\nSentiment: .+?\",\n",
    "    'template': \"\"\"Review: {example}\n",
    "Sentiment: {label}\"\"\",\n",
    "    'instruction': 'Choose sentiment from Positive or Negative .',\n",
    "}\n",
    "\n",
    "def sst2ICL_sample_factory(n_examples: int) -> List[ICLSample]:\n",
    "    sst2_template = Template(sst2_config['template'], sst2_config['instruction'])\n",
    "    records = []\n",
    "    # aggregate all records\n",
    "    for record in sst2_dataset['train']:\n",
    "        records.append({\n",
    "            'sentence': record['sentence'],\n",
    "            'label': 'Positive' if record['label'] == 1 else 'Negative',\n",
    "        })\n",
    "    for record in sst2_dataset['validation']:\n",
    "        records.append({\n",
    "            'sentence': record['sentence'],\n",
    "            'label': 'Positive' if record['label'] == 1 else 'Negative',\n",
    "        })\n",
    "    chunks = split_list_into_chunks(records, n_examples)\n",
    "    samples: List[ICLSample] = []\n",
    "    ground_truth: List[str] = []\n",
    "    for chunk in chunks:\n",
    "        examples = [example['sentence'] for example in chunk[:-1]]\n",
    "        labels = [example['label'] for example in chunk[:-1]]\n",
    "        test_sample = chunk[-1]['sentence']\n",
    "        ground_truth.append(chunk[-1]['label'])\n",
    "        sample: ICLSample = ICLSample(examples, labels, sst2_template, test_sample)\n",
    "        samples.append(sample)\n",
    "    return samples, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_data = sst2ICL_sample_factory(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trec (d:/Cyber-final-project/data/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2)\n",
      "100%|██████████| 2/2 [00:00<00:00, 401.25it/s]\n"
     ]
    }
   ],
   "source": [
    "trec_dataset = load_dataset(\"trec\", cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_config = {\n",
    "    'pattern': r\"Question: .+?\\nAnswer: .+?\",\n",
    "    'template': \"\"\"Question: {example}\n",
    "Answer: {label}\"\"\",\n",
    "    'instruction': 'Classify the questions based on whether their answer type is a Number, Location, Person, Description, Entity, or Abbreviation.\\n',\n",
    "}\n",
    "\n",
    "def trecICL_sample_factory(n_examples: int) -> List[ICLSample]:\n",
    "    trec_template = Template(trec_config['template'], trec_config['instruction'])\n",
    "    numeric_label_2_textual = {\n",
    "        0: 'Abbreviation',\n",
    "        1: 'Entity',\n",
    "        2: 'Description',\n",
    "        3: 'Person',\n",
    "        4: 'Location',\n",
    "        5: 'Number',\n",
    "    }\n",
    "    records = []\n",
    "    # aggregate all records\n",
    "    for record in trec_dataset['train']:\n",
    "        records.append({\n",
    "            'text': record['text'],\n",
    "            'label': numeric_label_2_textual[record['coarse_label']],\n",
    "        })\n",
    "    for record in trec_dataset['test']:\n",
    "        records.append({\n",
    "            'text': record['text'],\n",
    "            'label': numeric_label_2_textual[record['coarse_label']],\n",
    "        })\n",
    "    chunks = split_list_into_chunks(records, n_examples)\n",
    "    samples: List[ICLSample] = []\n",
    "    ground_truth: List[str] = []\n",
    "    for chunk in chunks:\n",
    "        examples = [example['text'] for example in chunk[:-1]]\n",
    "        labels = [example['label'] for example in chunk[:-1]]\n",
    "        test_sample = chunk[-1]['text']\n",
    "        ground_truth.append(chunk[-1]['label'])\n",
    "        sample: ICLSample = ICLSample(examples, labels, trec_template, test_sample)\n",
    "        samples.append(sample)\n",
    "    return samples, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_data = trecICL_sample_factory(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    }
   ],
   "source": [
    "# why?\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "huggingface_model = HuggingFaceModelWrapper(model=model, tokenizer=tokenizer)\n",
    "attack = ICLTextBugger.build(huggingface_model, sst2_config['pattern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_samples, sst_labels = sst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 594/594 [00:00<00:00, 149kB/s]\n",
      "c:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Cyber-final-project\\models\\clf-llama. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 22.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 413kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 700/700 [00:00<00:00, 428kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<?, ?B/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [03:10<00:00, 52.5MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:47<00:00, 74.4MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [03:58<00:00, 119.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:27<00:00, 43.69s/it]\n",
      "Some weights of the model checkpoint at huggyllama/llama-7b were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at huggyllama/llama-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n"
     ]
    }
   ],
   "source": [
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_labels = 2\n",
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "clf_model_config = LlamaConfig.from_pretrained(pretrained_model_name_or_path=\"huggyllama/llama-7b\", num_labels=n_labels, cache_dir=\"./models/clf-llama\")\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "clf_tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path=\"huggyllama/llama-7b\", cache_dir=\"./models/clf-llama\")\n",
    "# default to left padding\n",
    "clf_tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "clf_tokenizer.pad_token = clf_tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "clf_model = LlamaForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"huggyllama/llama-7b\", config=clf_model_config, cache_dir=\"./models/clf-llama\")\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "clf_model.resize_token_embeddings(len(clf_tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "clf_model.config.pad_token_id = clf_model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "clf_model.to(device)\n",
    "print('Model loaded to `%s`'%device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attack\u001b[39m.\u001b[39;49mattack(sst_samples[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto_text(), sst_labels[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\attack.py:442\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    437\u001b[0m     example \u001b[39m=\u001b[39m AttackedText(example)\n\u001b[0;32m    439\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    440\u001b[0m     ground_truth_output, (\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    441\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39m`ground_truth_output` must either be `str` or `int`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 442\u001b[0m goal_function_result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoal_function\u001b[39m.\u001b[39;49minit_attack_example(\n\u001b[0;32m    443\u001b[0m     example, ground_truth_output\n\u001b[0;32m    444\u001b[0m )\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m goal_function_result\u001b[39m.\u001b[39mgoal_status \u001b[39m==\u001b[39m GoalFunctionResultStatus\u001b[39m.\u001b[39mSKIPPED:\n\u001b[0;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:68\u001b[0m, in \u001b[0;36mGoalFunction.init_attack_example\u001b[1;34m(self, attacked_text, ground_truth_output)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mground_truth_output \u001b[39m=\u001b[39m ground_truth_output\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 68\u001b[0m result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result(attacked_text, check_skip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m result, _\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:79\u001b[0m, in \u001b[0;36mGoalFunction.get_result\u001b[1;34m(self, attacked_text, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_result\u001b[39m(\u001b[39mself\u001b[39m, attacked_text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     77\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A helper method that queries ``self.get_results`` with a single\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m    ``AttackedText`` object.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     results, search_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_results([attacked_text], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     result \u001b[39m=\u001b[39m results[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(results) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m result, search_over\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:96\u001b[0m, in \u001b[0;36mGoalFunction.get_results\u001b[1;34m(self, attacked_text_list, check_skip)\u001b[0m\n\u001b[0;32m     94\u001b[0m     attacked_text_list \u001b[39m=\u001b[39m attacked_text_list[:queries_left]\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(attacked_text_list)\n\u001b[1;32m---> 96\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_model(attacked_text_list)\n\u001b[0;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m attacked_text, raw_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(attacked_text_list, model_outputs):\n\u001b[0;32m     98\u001b[0m     displayed_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_displayed_output(raw_output)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:216\u001b[0m, in \u001b[0;36mGoalFunction._call_model\u001b[1;34m(self, attacked_text_list)\u001b[0m\n\u001b[0;32m    210\u001b[0m         uncached_list\u001b[39m.\u001b[39mappend(text)\n\u001b[0;32m    211\u001b[0m uncached_list \u001b[39m=\u001b[39m [\n\u001b[0;32m    212\u001b[0m     text\n\u001b[0;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m attacked_text_list\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m text \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_model_cache\n\u001b[0;32m    215\u001b[0m ]\n\u001b[1;32m--> 216\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_model_uncached(uncached_list)\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m text, output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(uncached_list, outputs):\n\u001b[0;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_model_cache[text] \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:165\u001b[0m, in \u001b[0;36mGoalFunction._call_model_uncached\u001b[1;34m(self, attacked_text_list)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(inputs):\n\u001b[0;32m    164\u001b[0m     batch \u001b[39m=\u001b[39m inputs[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[1;32m--> 165\u001b[0m     batch_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Some seq-to-seq models will return a single string as a prediction\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# for a single-string list. Wrap these in a list.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_preds, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\models\\wrappers\\huggingface_model_wrapper.py:61\u001b[0m, in \u001b[0;36mHuggingFaceModelWrapper.__call__\u001b[1;34m(self, text_input_list)\u001b[0m\n\u001b[0;32m     58\u001b[0m inputs_dict\u001b[39m.\u001b[39mto(model_device)\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 61\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs_dict)\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[0;32m     64\u001b[0m     \u001b[39m# HuggingFace sequence-to-sequence models return a list of\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# string predictions as output. In this case, return the full\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# list of outputs.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "attack.attack(sst_samples[0].to_text(), sst_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attack\u001b[39m.\u001b[39;49mattack(sst_samples[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto_text(), sst_labels[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\attack.py:442\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    437\u001b[0m     example \u001b[39m=\u001b[39m AttackedText(example)\n\u001b[0;32m    439\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    440\u001b[0m     ground_truth_output, (\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    441\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39m`ground_truth_output` must either be `str` or `int`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 442\u001b[0m goal_function_result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoal_function\u001b[39m.\u001b[39;49minit_attack_example(\n\u001b[0;32m    443\u001b[0m     example, ground_truth_output\n\u001b[0;32m    444\u001b[0m )\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m goal_function_result\u001b[39m.\u001b[39mgoal_status \u001b[39m==\u001b[39m GoalFunctionResultStatus\u001b[39m.\u001b[39mSKIPPED:\n\u001b[0;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:68\u001b[0m, in \u001b[0;36mGoalFunction.init_attack_example\u001b[1;34m(self, attacked_text, ground_truth_output)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mground_truth_output \u001b[39m=\u001b[39m ground_truth_output\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 68\u001b[0m result, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result(attacked_text, check_skip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m result, _\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:79\u001b[0m, in \u001b[0;36mGoalFunction.get_result\u001b[1;34m(self, attacked_text, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_result\u001b[39m(\u001b[39mself\u001b[39m, attacked_text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     77\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A helper method that queries ``self.get_results`` with a single\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m    ``AttackedText`` object.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     results, search_over \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_results([attacked_text], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     result \u001b[39m=\u001b[39m results[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(results) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m result, search_over\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:96\u001b[0m, in \u001b[0;36mGoalFunction.get_results\u001b[1;34m(self, attacked_text_list, check_skip)\u001b[0m\n\u001b[0;32m     94\u001b[0m     attacked_text_list \u001b[39m=\u001b[39m attacked_text_list[:queries_left]\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_queries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(attacked_text_list)\n\u001b[1;32m---> 96\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_model(attacked_text_list)\n\u001b[0;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m attacked_text, raw_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(attacked_text_list, model_outputs):\n\u001b[0;32m     98\u001b[0m     displayed_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_displayed_output(raw_output)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:216\u001b[0m, in \u001b[0;36mGoalFunction._call_model\u001b[1;34m(self, attacked_text_list)\u001b[0m\n\u001b[0;32m    210\u001b[0m         uncached_list\u001b[39m.\u001b[39mappend(text)\n\u001b[0;32m    211\u001b[0m uncached_list \u001b[39m=\u001b[39m [\n\u001b[0;32m    212\u001b[0m     text\n\u001b[0;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m attacked_text_list\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m text \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_model_cache\n\u001b[0;32m    215\u001b[0m ]\n\u001b[1;32m--> 216\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_model_uncached(uncached_list)\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m text, output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(uncached_list, outputs):\n\u001b[0;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_model_cache[text] \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\goal_functions\\goal_function.py:165\u001b[0m, in \u001b[0;36mGoalFunction._call_model_uncached\u001b[1;34m(self, attacked_text_list)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(inputs):\n\u001b[0;32m    164\u001b[0m     batch \u001b[39m=\u001b[39m inputs[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[1;32m--> 165\u001b[0m     batch_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Some seq-to-seq models will return a single string as a prediction\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# for a single-string list. Wrap these in a list.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_preds, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\textattack\\models\\wrappers\\huggingface_model_wrapper.py:61\u001b[0m, in \u001b[0;36mHuggingFaceModelWrapper.__call__\u001b[1;34m(self, text_input_list)\u001b[0m\n\u001b[0;32m     58\u001b[0m inputs_dict\u001b[39m.\u001b[39mto(model_device)\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 61\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs_dict)\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[0;32m     64\u001b[0m     \u001b[39m# HuggingFace sequence-to-sequence models return a list of\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# string predictions as output. In this case, return the full\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# list of outputs.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "attack.attack(sst_samples[0].to_text(), sst_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose sentiment from Positive or Negative .\n",
      "\n",
      "Review: hide new secretions from the parental units \n",
      "Sentiment: Negative\n",
      "Review: contains no wit , only labored gags \n",
      "Sentiment: Negative\n",
      "Review: that loves its characters and communicates something rather beautiful about human nature \n",
      "Sentiment: Positive\n",
      "Review: remains utterly satisfied to remain the same throughout \n",
      "Sentiment: _\n"
     ]
    }
   ],
   "source": [
    "print(sst_samples[0].to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temp = np.array([\"\"\"Choose sentiment from Positive or Negative .\n",
    "\n",
    "Review: hide new secretions from the parental units \n",
    "Sentiment: _\"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model(temp)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mpredict(temp)\n",
      "File \u001b[1;32mc:\\Users\\Mark\\anaconda3\\envs\\cyber\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# model(temp)\n",
    "model.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    \"The capital of Canada is\",\n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "# batch = {k: v.to(device) for k, v in batch.items()}\n",
    "generated = model.generate(batch[\"input_ids\"], max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  450,  7483,   310,  7400,   338, 13476, 10011, 29889,    13,  6028,\n",
       "          1114,   338,   263,  4234,   297,  4644,  6813, 29889,   739,   338,\n",
       "           278,  1473, 10150,  4234,   297,   278,  3186, 29889,  7400,   338,\n",
       "          5139,   287,   491,   278,  3303,  3900,   304,   278,  7062,   322,\n",
       "           278, 14328, 21091,   304,   278,  5833, 29889,  7400,   756,   263,\n",
       "          4665,   310, 29871, 29941, 29945,  7284,  2305, 29889,   450,  7483,\n",
       "           310,  7400,   338, 13476, 10011, 29889,    13,  6028,  1114,   338,\n",
       "           263,  4234,   297,  4644,  6813, 29889,   739,   338,   278,  1473,\n",
       "         10150,  4234,   297,   278,  3186, 29889,  7400,   338,  5139,   287,\n",
       "           491,   278,  3303,  3900,   304,   278,  7062,   322,   278, 14328]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_temp = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_scores = model.generate(batch[\"input_ids\"], max_length=100, return_dict_in_generate=True, output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequences', 'scores']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generated_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  450,  7483,   310,  7400,   338, 13476, 10011, 29889,    13,  6028,\n",
       "         1114,   338,   263,  4234,   297,  4644,  6813, 29889,   739,   338,\n",
       "          278,  1473, 10150,  4234,   297,   278,  3186, 29889,  7400,   338,\n",
       "         5139,   287,   491,   278,  3303,  3900,   304,   278,  7062,   322,\n",
       "          278, 14328, 21091,   304,   278,  5833, 29889,  7400,   756,   263,\n",
       "         4665,   310, 29871, 29941, 29945,  7284,  2305, 29889,   450,  7483,\n",
       "          310,  7400,   338, 13476, 10011, 29889,    13,  6028,  1114,   338,\n",
       "          263,  4234,   297,  4644,  6813, 29889,   739,   338,   278,  1473,\n",
       "        10150,  4234,   297,   278,  3186, 29889,  7400,   338,  5139,   287,\n",
       "          491,   278,  3303,  3900,   304,   278,  7062,   322,   278, 14328])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_scores['sequences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated_scores['sequences']\n",
    "len(tokenizer.decode(generated_scores['sequences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-15.8295)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_scores['scores'][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 450, 7483,  310, 7400,  338]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  450,  7483,   310,  7400,   338, 13476, 10011, 29889,    13,  6028,\n",
       "          1114,   338,   263,  4234,   297,  4644,  6813, 29889,   739,   338,\n",
       "           278,  1473, 10150,  4234,   297,   278,  3186, 29889,  7400,   338,\n",
       "          5139,   287,   491,   278,  3303,  3900,   304,   278,  7062,   322,\n",
       "           278, 14328, 21091,   304,   278,  5833, 29889,  7400,   756,   263,\n",
       "          4665,   310, 29871, 29941, 29945,  7284,  2305, 29889,   450,  7483,\n",
       "           310,  7400,   338, 13476, 10011, 29889,    13,  6028,  1114,   338,\n",
       "           263,  4234,   297,  4644,  6813, 29889,   739,   338,   278,  1473,\n",
       "         10150,  4234,   297,   278,  3186, 29889,  7400,   338,  5139,   287,\n",
       "           491,   278,  3303,  3900,   304,   278,  7062,   322,   278, 14328]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.decode(generated[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyber-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
